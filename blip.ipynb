{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import TFAutoModelForSeq2SeqLM, TFAutoModel, TFBlipForConditionalGeneration\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor and model from Huggingface\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Convert model to TensorFlow model (optional, but necessary for full TensorFlow-based modifications)\n",
    "\n",
    "tf_model = TFBlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test.csv')\n",
    "img_id = [str(x) for x in data['id'].values.tolist()]\n",
    "units = [str(x) for x in data['unit'].values.tolist()]\n",
    "entity_name = [str(x) for x in data['entity_name'].values.tolist()]\n",
    "numeric_values = [str(x) for x in data['numeric_value'].values.tolist()]\n",
    "numeric_values = processor.tokenizer(text = numeric_values, padding=True, truncation=True, max_length=8, return_tensors=\"tf\")\n",
    "units = processor.tokenizer(text = units, padding=True, truncation=True, max_length=20, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CustomDataset(tf.data.Dataset):\n",
    "    def __new__(cls, img_ids, entity_names, units, numeric_values):\n",
    "        # Create the dataset from the provided lists or arrays\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((img_ids, entity_names, units, numeric_values))\n",
    "        \n",
    "        # Apply the mapping function using the load_dataset\n",
    "        dataset = dataset.map(\n",
    "            lambda img_id, entity_name, units, numeric_value: cls.map_fn(img_id, entity_name, units, numeric_value),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(img_id, entity_name, units, numeric_value):\n",
    "        img_dir = 'datasets/test_images/'\n",
    "        path = img_dir + img_id + '.jpg'\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (224, 224))\n",
    "        img = img / 255.0  # Normalize\n",
    "\n",
    "        return img, entity_name, units, numeric_value\n",
    "\n",
    "    @staticmethod\n",
    "    def process_text(entity_name, img):\n",
    "        # The processor function, executed dynamically\n",
    "        def processor_fn(entity_name_str, img_tensor):\n",
    "            # Assuming 'processor' is a pre-defined object\n",
    "            encoding = processor(image=img_tensor, text=entity_name_str.decode('utf-8'), return_tensors=\"tf\", padding=True, truncation=True)\n",
    "            return encoding['pixel_values'][0], encoding['input_ids'][0], encoding['attention_mask'][0]\n",
    "        \n",
    "        # Using tf.py_function to wrap Python's processor function\n",
    "        img, input_ids, attention_mask = tf.py_function(\n",
    "            func=processor_fn,\n",
    "            inp=[entity_name, img],\n",
    "            Tout=[tf.float32, tf.int32, tf.int32]\n",
    "        )\n",
    "        return img, input_ids, attention_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def map_fn(img_id, entity_name, units, numeric_value):\n",
    "        img, entity_name, units, numeric_value = CustomDataset.load_dataset(img_id, entity_name, units, numeric_value)\n",
    "        img, input_ids, attention_mask = CustomDataset.process_text(entity_name, img)\n",
    "        return {\n",
    "            \"pixel_values\": img,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels_units\": units,\n",
    "            \"labels_numeric_values\": numeric_value\n",
    "        } \n",
    "\n",
    "\n",
    "\n",
    "custom_dataset = CustomDataset(img_id, entity_name, units, numeric_values)\n",
    "\n",
    "# Prepare the dataset for batching and optimization\n",
    "custom_dataset = custom_dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_blip_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vision_model (TFBlipVision  multiple                  86090496  \n",
      " Model)                                                          \n",
      "                                                                 \n",
      " text_decoder (TFBlipTextLM  multiple                  161323580 \n",
      " HeadModel)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 247414076 (943.81 MB)\n",
      "Trainable params: 247414076 (943.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tf_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (561471154.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.word_output = tf.keras.layers.Dense(units=base_model.config., activation='softmax', name='word_output')\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NUMERIC_VOCAB_SIZE = 11\n",
    "\n",
    "# Modify the numeric output head to generate a sequence of 7 characters\n",
    "class CustomBlipModel(tf.keras.Model):\n",
    "    def __init__(self, base_model, max_numeric_length=7):\n",
    "        super(CustomBlipModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.max_numeric_length = max_numeric_length\n",
    "        \n",
    "        # Define a separate head for the numeric output (predicting a sequence of length 7)\n",
    "        self.numeric_output = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(units=NUMERIC_VOCAB_SIZE, activation='softmax'),\n",
    "            name='numeric_output'\n",
    "        )\n",
    "        \n",
    "        # Word output head\n",
    "        self.word_output = tf.keras.layers.Dense(units=base_model., activation='softmax', name='word_output')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Get the base model output\n",
    "        base_output = self.base_model(inputs)[0]  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Split into numeric and word outputs\n",
    "        numeric_output = self.numeric_output(base_output[:, :self.max_numeric_length, :])  # Sequence for numeric string\n",
    "        word_output = self.word_output(base_output[:, self.max_numeric_length:, :])  # For word tokens\n",
    "        \n",
    "        return numeric_output, word_output\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = CustomBlipModel(tf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "\n",
    "# Define the numeric vocabulary size (10 digits + 1 decimal point)\n",
    "NUMERIC_VOCAB_SIZE = 11\n",
    "\n",
    "# Modify the numeric output head to generate a sequence of 7 characters\n",
    "class CustomBlipModel(tf.keras.Model):\n",
    "    def __init__(self, base_model, max_numeric_length=7):\n",
    "        super(CustomBlipModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.max_numeric_length = max_numeric_length\n",
    "        \n",
    "        # Define a separate head for the numeric output (predicting a sequence of length 7)\n",
    "        self.numeric_output = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(units=NUMERIC_VOCAB_SIZE, activation='softmax'),\n",
    "            name='numeric_output'\n",
    "        )\n",
    "        \n",
    "        # Word output head\n",
    "        self.word_output = tf.keras.layers.Dense(units=base_model.config.vocab_size, activation='softmax', name='word_output')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Get the base model output\n",
    "        base_output = self.base_model(inputs)[0]  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Split into numeric and word outputs\n",
    "        numeric_output = self.numeric_output(base_output[:, :self.max_numeric_length, :])  # Sequence for numeric string\n",
    "        word_output = self.word_output(base_output[:, self.max_numeric_length:, :])  # For word tokens\n",
    "        \n",
    "        return numeric_output, word_output\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = CustomBlipModel(tf_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
